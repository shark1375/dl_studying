{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. 파이토치에서의 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.1919e-10, 2.0447e+23, 1.3664e+22],\n",
       "        [4.1914e+21, 5.1250e-11, 4.2330e+21]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텐서를 생성하는 가장 간단한 방식\n",
    "# 이렇게 생성하면, array처럼 2,3이 원소로 들어가는 게 아니라 2x3차원의 텐서를 생성함\n",
    "X = torch.Tensor(2,3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 값을 넣을거면 Tensor가 아닌 tensor 클래스를 호출\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인자로 data, dtype, device, requrires_grad(기울기를 저장할지)등을 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.tensor(data = [2.0, 3.0], requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기울기를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 12.])\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-0a9005ac6ff1>:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(y.grad) #잎 노드 X\n",
      "<ipython-input-19-0a9005ac6ff1>:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(z.grad) #잎 노드 X\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(data = [2., 3.], requires_grad = True)\n",
    "y = x**2\n",
    "z = 2*y + 3\n",
    "# 연산 대상 식은 z = 2x^2 + 3\n",
    "\n",
    "target = torch.tensor([3., 4.])\n",
    "loss = torch.sum(torch.abs(z - target))\n",
    "loss.backward() # 연산 그래프를 따라가며 잎노드인 x에 대한 기울기를 계산\n",
    "\n",
    "print(x.grad) #잎 노드\n",
    "print(y.grad) #잎 노드 X\n",
    "print(z.grad) #잎 노드 X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형회귀분석 모델을 만들어 기울기를 계산하고 w, b를 업데이트까지 하는 코드를 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn # 신경망 모델들이 포함되어있는 클래스\n",
    "from torch import optim # 옵티마이저 : 경사하강 알고리즘들이 들어있음\n",
    "from torch.nn import init # 초기화 방식을 결정할 수 있는 함수들이 포함된 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 1000 # 데이터 수\n",
    "num_epoch = 500 # 경사하강 횟수\n",
    "\n",
    "x = init.uniform_(torch.Tensor(num_data, 1), -10, 10)\n",
    "    # -10 ~ 10의 유니폼 분포로 초기화 전략 사용\n",
    "    # 랜덤하게 초기화하기 때문에 Tensor를 사용하며 입력값으로, input_shape를 넣어줌\n",
    "noise = init.normal_(torch.FloatTensor(num_data, 1), std = 1) # 현실성 반영을 위해 이번 테스트에서는 정규분포의 가우시안 노이즈 추가\n",
    "y = 2*x + 3\n",
    "y_noise = y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "loss_func = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear 클래스는 input feature의 수와 결과로 나오는 output feature, bias 사용 여부를 초기화 인자로 받음\n",
    "    - 지금은 1개의 특성을 가진 1000개의 데이터를 입력으로 넣었기에 input_shape = (1000, 1), input_feature = 1\n",
    "    - output으로도 1개의 특성을 가진 1000개의 데이터를 받을 예정. 따라서 output_shape = (1000, 1), output_feature = 1\n",
    "- L1손실 : loss(x, y) = 1/n * SIGMA(|x_i - y_i|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.8562]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1511], requires_grad=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성한 모델에서 학습이 필요한 파라메터들을 담고 있는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사하강법 사용을 위해 SGD 옵티마이저 이용\n",
    "- 옵티마이저 : 최적화 함수라고도 불리며, 경사하강법을 적용하여 오차를 줄이고 최적의 w, b를 근사할 수 있도록 하는 역할\n",
    "- 옵티마이저의 인자로 모델에서 학습 대상이될 파라메터들을 입력하고 learning_rate 입력\n",
    "    - 따라서 model.parameters() 메서드를 이용\n",
    "    - 이렇게되면, w와 b가 학습됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0180)\n",
      "tensor(3.2893)\n",
      "tensor(2.9079)\n",
      "tensor(2.7090)\n",
      "tensor(2.5158)\n",
      "tensor(2.3248)\n",
      "tensor(2.1357)\n",
      "tensor(1.9494)\n",
      "tensor(1.7740)\n",
      "tensor(1.6092)\n",
      "tensor(1.4575)\n",
      "tensor(1.3272)\n",
      "tensor(1.2122)\n",
      "tensor(1.1112)\n",
      "tensor(1.0290)\n",
      "tensor(0.9683)\n",
      "tensor(0.9209)\n",
      "tensor(0.8862)\n",
      "tensor(0.8603)\n",
      "tensor(0.8406)\n",
      "tensor(0.8254)\n",
      "tensor(0.8140)\n",
      "tensor(0.8050)\n",
      "tensor(0.7986)\n",
      "tensor(0.7939)\n",
      "1.9994646310806274 2.8638575077056885\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "\n",
    "label = y_noise\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    optimizer.zero_grad() # 각 반복에서 지난 에포크에서 계산한 기울기를 0으로 초기화하는 과정\n",
    "    output = model(x) # 선형 모델로부터 나온 결과값\n",
    "    \n",
    "    loss = loss_func(output, label) # y_pred와 y_true를 비교하여 loss 생성\n",
    "    loss.backward() # 역전파를 통해 w와 b를 학습시킴\n",
    "    optimizer.step() # learning_rate만큼 전진하여 model.parametes()에서 리턴되는 변수들의 기울기에 학습률을 적용\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(loss.data)\n",
    "        \n",
    "param_list = list(model.parameters())\n",
    "print(param_list[0].item(), param_list[1].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
