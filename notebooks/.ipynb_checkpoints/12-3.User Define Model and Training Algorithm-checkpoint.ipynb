{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.3. 사용자 정의 모델과 훈련 알고리즘\n",
    "- 가장 간단하고 많이 사용하는 사용자 정의 손실함수를 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1. 사용자 정의 손실함수\n",
    "- 만약 회귀모델을 훈련하는데 훈련 세트에 잡음이 있다고 가정\n",
    "- 이상치를 제거하거나 고쳐서 데이터셋을 수정해보는 방법이 가장 원론적임\n",
    "    - 하지만 비효율적이고 잡음을 백프로 제거하기는 어려움\n",
    "- 이럴 때는 손실함수를 바꿔서 시도하는 것도 괜춘함\n",
    "    - 1) MSE : 큰 오차에 너무 큰 벌칙을 가하기 때문에 정확도가 떨어질 것\n",
    "    - 2) MAE : 이상치에 너무 관대하여 최적값을 찾는 데 시간이 오래걸릴것이며 정밀도가 떨어질 것\n",
    "- 이럴 때 사용하기 좋은 것이 **후버 손실**임\n",
    "    - 아직 keras 공식 API지원은 없고, tf.keras.losses.Huber에서 지원하긴 함\n",
    "- 이를 구현하려면 아래와 같이 레이블과 예측을 매개변수로 받는 함수를 만들고 텐서플로 연산을 이용하여 손실을 계산하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 때 주의할 점은 **전체 손실의 평균이 아니라, 하나의 손실을 담은 텐서를 반환하는 것이 좋음**\n",
    "    - 이렇게 해야 필요할 때 케라스가 클래스 가중치나 샘플 가중치를 적용할 수 있음\n",
    "- 해당 손실함수를 이용하여 컴파일하는 방식은 아래와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss = huber_fn, optimizer = \"nadam\")\n",
    "# model.fit(X_tr, y_tr, [...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련은 이런 식으로 매우 간단하게 할 수 있음\n",
    "- 하지만 모델을 저장하고 로드할 때는 문제가 발생할 수 있다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2. 사용자 정의 요소를 가진 모델을 저장하고 로드하기\n",
    "- 케라스가 함수 이름을 저장하므로, 사실 저장 자체에는 문제가 없음\n",
    "- 하지만, 모델을 로드할 때는 함수 이름과 실제 함수를 매핑한 **딕셔너리 객체**를 전달해야함\n",
    "- 좀 더 일반적으로 정리하자면, 사용자 정의 객체를 포함한 모델을 로드할 때는 그 이름과 객체를 **매핑**해줘야함\n",
    "    - model을 load해올 때 custom_object 인자를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# model = keras.models.load_models(\"my_model.h5\", custom_object = {\"huber_fn\" : huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런데 위와 같이 후버 함수를 만들어서 저장할 때, \"작은 것\"의 기준을 따로 threshold로 정하고 싶을 수 있음\n",
    "- 그럴 때는 huber_fn의 함수를 정의하는 함수를 만들어 threshold를 인자로 건내주면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold = 1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = tf.abs(error) - threshold**2/2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)        \n",
    "    return huber_fn\n",
    "\n",
    "# model.compile(loss = create_huber(2.0), optimizer = \"nadam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런데 이렇게 했을 때 아까처럼 모델로딩 시 문제가 또 발생하는데, threshold값이 저장되지 않기 때문임\n",
    "- 그래서 모델을 로드할 때 위에서 custom_object를 잡아줬던 것 처럼 똑같이 threshold도 잡아줘야함\n",
    "    - *json으로 모델 훈련시 썼던 하이퍼파라미터들을 다 저장하는 것이 좋을 것 같음...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.load_model(\"my_model.h5\", custom_obejct   = {\"huber_fn\" : create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고로 새로 정의한 함수가 아닌, 컴파일 할 때 생성이 된 fn의 이름을 custom_object로 전달해야함\n",
    "    - 즉 컴파일할때는 새로 정의된 함수를 썼지만, 결국 리턴된 값은 huber_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 문제는 keras.losses.Loss클래스를 상속하고 get_config()메서드를 구현하여 해결할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "\n",
    "    def __init__(self, threshold = 1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_samll_error, square_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\" : self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이렇게하면 컴파일도 쉬워지며, 저장과정에서 threshold도 함께 저장이됨\n",
    "- 대신 모델 로드 시 **클래스 이름**과 **클래스 자체**를 매핑시켜줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss = HuberLoss(2.), optimizer=  \"nadam\")\n",
    "# model = kears.models.load_model(\"my_model.h5\", custom_objects = {\"HuberLoss\" : HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델을 저장할 떄 케라스가 손실 객체의 get_config()메서드를 호출해서 반환된 설정을 HDF5파일에 json형태로 저장\n",
    "    - 그래서 모델을 로딩할 때 HuberLoss클래스의 (상속된 메서드인) from_config()클래스 메서드를 호출함\n",
    "    - 이 메서드는 기본 손실 클래스에 구현되어있고 생성자에 \\*\\*config매개변수를 전달해서 클래스 인스턴스를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.3. 활성화 함수, 초기화, 규제, 제한 커스터마이징\n",
    "- 대부분 위와 유사한 방식으로 커스터마이징 가능\n",
    "    - 보통은 적절한 입/출력을 가진 간단한 함수를 작성하면 됨\n",
    "- 다음은 사용자 정의 softplus활성화함수, 글로럿 초기화, L1규제, 정의 제한(양수인 가중치만 남기는 것)에 대한 예\n",
    "    - 위 케이스들은 각각 keras.activations.softplus(), tf.nn.softplus()\n",
    "    - keras.initializers.glorot_normal\n",
    "    - keras.regularizers.l1(0.01)\n",
    "    - keras.constraints.nonneg(), tf.nn.relu()와 동일함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplust(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype = tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev = stddev, dtype = dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 보듯 매개변수는 사용자가 정의하려는 함수에 따라 다름\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
