{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 깊은 신경망을 구성하는 것은 매우 어려운 일 ㅠㅠ\n",
    "    - 까다로운 **그래디언트 소실**혹은 **그래디언트 폭주** 문제에 젝면할 수 있음\n",
    "    - 데이터 자체가 부족하지 않거나, 데이터를 레이블링하는 것 자체가 큰 코스트일 수 있음\n",
    "    - 훈련 속도가 극단적으로 느려질 수 있음\n",
    "    - 오버피팅의 가능성이 큼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.1. 그래디언트 소실과 폭주 문제\n",
    "- 역전파 알고리즘은 출력층에서 입력층으로 오차 그래디언트를 전파하고, 알고리즘이 오차함수의 그래디언트를 계산하면, 경사하강 단계에서 이 그래디언트를 이용하여 각 파라미터를 수정하는 원리\n",
    "    - 그런데 이 떄 하위층으로 알고리즘이 진행될 수록 그래디언트가 점차 작아지는 경우가 많음(**그래디언트 소실**)\n",
    "    - 따라서 그냥 둘 경우 하위층의 연결 가중치가 업데이트되지 않을 것이며, 이 경우 제대로된 솔루션이 될 수 없음\n",
    "    - 오히려 반대로 그래디언트가 너무 커져서 각 층이 비정상적으로 큰 가중치로 갱신되는 경우도 있음(**그래디언트 폭주**, 주로 RNN에서 발생)\n",
    "- 이러한 이유는 명확하지 않지만, 의심되는 것들이 몇가지가 있음\n",
    "    - 그 중에 하나는 로지스틱 시그모이드 활성화함수와 가중치 초기화방법(초기에는 가장 인기있었던 방법이 정규분포였음)의 조합이었음\n",
    "        - 이 활성화 함수와 초기화 방식을 사용할 경우 각 층에서 출력 분산이 입력 분산보다 더 크다는 것이 알려짐\n",
    "        - 결국 신경망의 위쪽으로 갈 수록 분산이 계속 커져 가장 높은 층에서는 활성화 함수가 0이나 1로 수렴하는 문제가 발생\n",
    "        - 게다가 로지스틱 함수의 경우 평균이 0.5이기 떄문에 더 나빠짐(하이퍼볼릭은 평균이 0이므로 좀 더 괜찮았음)\n",
    "            - 항상 로지스틱 함수가 양수를 뱉어내므로, 출력의 가중치 합이 입력보다 커질 가능성이 있었던 것이어씀!!\n",
    "        - 로지스틱함수는 입력의 절댓값이 커지면 0이나 1로 수렴하는데, 이 때 그래디언트가 거의 없는 상태가 되어버림\n",
    "            - 따라서 최상위층에서부터 역전파가 진행될 때 신호가 점점 약해져서 아래층에는 정보가 아예 도달하지 않는 문제가 발생한거심!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.1. 글로럿과 He초기화\n",
    "- 적절한 신호가 흐리기 위해서는 각 층의 출력과 분산이 입력에 대한 분산과 같아야하며, 역방향에서 층을 통과하기 전, 후의 그래디언트 분산이 동일해야함\n",
    "    - 이는 이론적으로는 fan in과 fan out(입력층과 출력층의 연결개수)이 동일해야만 보장됨\n",
    "    - 하지만 글로럿과 벤지오가 실전에서 매웆 잘 작동하는 것으로 입증된 대안을 제시\n",
    "- **세이비어 초기화(글로럿 초기화)**\n",
    "    - *logisitc activation, softmax, tanh를 쓸 때*\n",
    "    - 각 층의 연결 가중치를 sigma^2 = 1/fan_avg 인 정규분포나 r = sqrt(3/fan_avg)일 때 -r ~ r의 균등분포로 무작위 초기화    \n",
    "- **르쿤 초기화**\n",
    "    - *SELU를 쓸 때*\n",
    "    - 위에서 나온 식에서 fan_avg를 fain_in으로 바꿔줌\n",
    "- 다른 논문들도 나왔지만, 일반적으로 분산의 스케일링이나 fan_avg를 쓰느냐 fan_in을 쓰느냐에 차이만 있음\n",
    "- RELU활성화함수에 대한 초기화 전략을 논문 저자의 이름을 따서 **He 초기화**라고 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 케라스는 기본적으로 균등분포의 글로럿 초기화를 사용함\n",
    "    - 층을 만들 때, kernel_initializer = \"he_uniform\"을 인자로 대입하거나, kernel_initializer = \"he_normal\"을 인자로 대입하면 He 초기화도 사용할 수 있음\n",
    "    - keras.layers.Dense(10, activation = \"relu\", kernel_initializer = 'he_normal')\n",
    "- fan_in 대신에 fan_out기반의 He초기화를 사용하고 싶다면, keras.initializer 모듈의 VarianceScaling 클래스를 활용\n",
    "    - he_avg_init = keras.initializers.VarianceScaling(scale = 2, mode = \"fan_avg\", distribution = \"uniform\")\n",
    "    - keras.layers.Dense(10, activation = \"sigmoid\", kernel_initializer = he_agv_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2. 수렴하지 않는 활성화 함수\n",
    "- 활성화 함수를 잘못 선택할 경우 그래디언트의 소실이나 폭주로 이어질 수 있다는 것을 발견\n",
    "- 그 전에는 생물학적 뉴런의 행동을 본딴 시그모이드가 최선일 것이라 생각\n",
    "    - 버뜨 다른 활성화 함수가 DNN에서 더 잘 작동함을 발견하게 되었고, 특히 ReLU가 그 대표\n",
    "- **ReLU**의 경우 특정 양수값에 수렴하지 않는다는 큰 장점이 있었고, 연산도 단순하다는 큰 장점이 있었음\n",
    "    - 물론 ReLU의 경우에도 문제가 있음\n",
    "    - **죽은 ReLU** : 훈련하는 동안 일부 뉴런이 0 이외의 값을 출력하지 않음\n",
    "    - 어떤 경우에는(특히 learning rate를 크게 잡아놓은 경우) 뉴런 절반이 죽어있을 수도 있음\n",
    "    - 뉴런의 가중치가 바뀌어 훈련세트 내 모든 샘플에 대해 입력의 가중치 합이 음수가 되면 뉴런이 죽게 됨\n",
    "        - 가중치 합이 음수 -> ReLU의 그래디언트 : 0 -> 따라서 경사하강법이 더이상 작동하지 않게됨\n",
    "- 위 문제를 해결하기 위해 **LeakyReLU**와 같은 ReLU의 변종을 이용함\n",
    "    - 1) LeakyReLU\n",
    "        - LeakyReLU_a(Z) = max(az, z) : a가 새는 정도(leaky)의 정도를 정함\n",
    "        - 새는 정도라는 것이 z < 0일 때의 함수의 기울기이며, 일반적으로 0.01로 지정\n",
    "        - 그러면 모든 값을 0으로 반환하는 것이 아니기 때문에 LeakyReLU의 경우 죽을 가능성이 ㅇ벗어짐\n",
    "        - 실제 연구에서는 a를 0.2정도로 크게 잡는 것이 좋은것으로 결과를 보이고 있음\n",
    "    - 2) RReLU\n",
    "        - Randomized Leaky ReLU\n",
    "        - 규제의 역할도 수행하며, 꽤 잘 작동하는 것으로 보임\n",
    "    - 3) PReLU\n",
    "        - a를 학습시키는 Learky ReLU모델(Parametric Leaky ReLU)\n",
    "        - 데이터가 적을 때 오버피팅이 되는 문제가 있었음\n",
    "- **ELU**\n",
    "    - Exponential Linear Unit이라는 2015년작 따끈한 신작 활성화함수\n",
    "    - ReLU와 전반적으로 비슷한 특징을 보여주지만, 성능상으로는 ReLU류 함수들보다더 좋은 결과를 보여줌\n",
    "    - if z <0, ELU_a(z) = a(exp(z) - 1), else z\n",
    "    - 하지만 exponential함수를 사용하므로 ReLU류보다는 당연히 느릴 수밖에 없다는 점에 문제가 있음\n",
    "        - 훈련하는 동안에는 지수함수로 연산하더라도, 수렴 속도가 워낙에 빨라 느린 계산이 상쇄되었지만, 테스트시에는 이 속도가 문제가될 수 있음\n",
    "- **SELU**\n",
    "    - Scaled ELU의 약자로 2017년 논문에서 소개됨\n",
    "    - ELU의 변종이며, 완전 연결층만 쌓아 DNN을 만들고 모든 은닉층에 SELU를 적용할 경우, 네트워크가 자기정규화될 수 있다는 것을 보임\n",
    "        - 즉, 각 층의 출력이 평균 0, 표준편차 1을 유지하려는 경향이 있다는 것\n",
    "        - 이는 그래디언트 소실과 폭주 문제를 보여줄 수 있음\n",
    "    - 종종 좋은 성능을 보이는데, 자기 정규화가 일어나려면 몇 가지 조건이 있는 것이 문제\n",
    "        - 1) 입력 특성이 반드시 표준화(...? 애초에 여기서부터가...)\n",
    "        - 2) 모든 은닉층의 가중치는 르쿤 정규분포로 초기화 되어야함\n",
    "        - 3) 네트워크는 RNN이나 Skip Connection 등의 형태가 아닌 Sequential이어야함\n",
    "- *책 추천 활성화함수 적용 순서 : SELU > ELU > LeakyReLU > ReLU > tanh > logisitic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####LeakyRelU사용법 : 층을 우선 만들고, 적용하려는 층 뒤에 LeakyReLU층을 추가하듯 접근\n",
    "# ##########PRelu도 사용법 동일\n",
    "\n",
    "# model = keras.models.Sequential([\n",
    "#     [...]\n",
    "#     keras.layers.Dense(10, kernel_initializer = \"he_normal\")\n",
    "#     keras.layers.LeakyReLU(alpha = 0.2)\n",
    "#     [...]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####SELU사용법 : 층을 만들 때 activation = \"selu\", kernel_initializer = \"lecun_normal\"\n",
    "# layer = keras.layers.Dense(10, activation = \"selu\", kernel_initializer = \"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3. 배치 정규화\n",
    "- ELU나 ReLU변종 및 He 초기화를 사용하면 훈련 초기 단계에서 그래디언트 소실이나 폭주 문제를 크게 감소시킬 수 있으나, 훈련하는동안 다시 발생할 수 있음\n",
    "- 이를 해결하기 위해 제안된 것이 **배치정규화(BN)**\n",
    "- 각 층에서 활성화 함수를 통과하기 전이나 후에 모델에 연산을 하나 추가함\n",
    "    - 단순히 입력을 원점에 맞추고 정규화한다음, 각 층에서 두 개의 새로운 파라미터로 결과값의 스케일을 조정하고 이동\n",
    "    - 이 두개의 파라미터 중 하나는 스케일 조정에, 하나는 이동에 사용함\n",
    "- 많은 경우 신경망의 첫 층으로 배치 정규화를 추가하여 tr, val, t데이터의 표준화 과정을 생략함(sklearn.preprocessing.StandardScaler)\n",
    "    - 이 역할을 배치 정규화층이 대신함\n",
    "- 입력 데이터를 원점에 맞추고 정규화하려면, 알고리즘은 mean과 std_error를 추정해야함\n",
    "    - 이를 위해 현재의 미니배치에서 입력의 평균과 표준편차를 평가함\n",
    "- 전체 알고리즘은 아래와 같음\n",
    "    - mu_b = 1/m_b * Sigma(i =1 ,m_b)x(i)\n",
    "        - 미니배치 B에 대해 평가한 입력의 평균 벡터\n",
    "        - m_b : 미니배치에 있는 샘플 수\n",
    "    - sigma_b^2 = 1/m_b * Sigma(i - 1, m_b)(x(i)-mu_b)^2\n",
    "        - 미니배치에 대해 평가한 입력의 표준편차 벡터\n",
    "    - x_hat(i) = (x(i)-mu_b)/sqrt(sigma_b^2 + epsilon)\n",
    "        - 평균이 0이고 정규화된 샘플 i의 입력\n",
    "        - epsilon은 분모가 0이되는것을 막기 위한 아주 작은 숫자(안전을 위한 항)임. 보통 10^-5으로 잡음\n",
    "    - z(i) = gamma X x_hat(i) + beta\n",
    "        - 배치 정규화 연산의 출력으로 입력의 스케일을 조정하고 이동시킨 것\n",
    "        - gamma : 층의 출력 스케일 파라메터 벡터(개별 입력마다 스케일 파라메터가 있음)\n",
    "        - X : 원소별 곱셈\n",
    "        - beta : 층의 출력이동 파라미터 벡터로 개별 입력마다 스케일 파라미터가 있음. 각 입력은 이 파라미터만큼 이동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련하는 동안 배치정규화는 입력을 정규화하고 다음 입력을 위한 스케일을 조정하여 이동시킴\n",
    "- 그런데 문제는 테스트임\n",
    "- 지금까지 모든 계산의 대상은 기본적으로 미니배치였음\n",
    "    - 그런데 예측 상황에서는 샘플의 배치가 아니라 샘플 하나에 대한 예측을 만들어야함\n",
    "- 이 경우 배치가 아니다보니 입력의 평균과 표준편차를 계산할 방법이 없음\n",
    "    - 샘플의 배치를 사용한다고 하더라도, 새로 모집된 데이터라 매우 작거나 독립 동일 분포(IID)조건을 만족하지 못할 수 있음\n",
    "    - 이런 샘플 배치라면, 계산이 되더라도 신뢰도가 떨어질 것\n",
    "- 한 가지 방법은 훈련이 끝난 후 전체 훈련 세트를 신경망에 통과시켜 배치 정규화 층의 각 입력에 대한 평균과 표준편차를 미릭 계산해놓는 것\n",
    "    - 그래서 예측 시 배치 입력과 표준 편차로 위에서 계산해놓은 값으로 대체할 수 있음\n",
    "- 그러나 대부분 배치 정규화 구현은 층의 입력평균과 표준편차의 이동 평균을 사용해 훈련하는 동안 최종 통계치를 추정함\n",
    "    - 케라스의 BatchNormalization층이 이를 자동으로 수행함\n",
    "- 다시 정리하면!!\n",
    "    - 1) 배치 정규화 층마다 네 개의 파라미터 벡터가 학습됨\n",
    "        - gamma : 출력 스케일 벡터\n",
    "        - beta : 출력 이동 벡터\n",
    "            - 위 두 파라미터는 일반적인 역전파를 통해 학습\n",
    "        - mu : 최종 입력 평균 벡터\n",
    "        - sigma : 최종 입력 표준편차 벡터\n",
    "            - 위 두 파라미터는 지수 이동평균을 사용하여 추정됨\n",
    "        - mu와 sigma는 ㅈ훈련하는동안 추정되지만, 실제로 사용은 훈련이 끝난 후에 사용됨(예측 과정에서)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이러한 배치 정규화로 인해 그래디언트 소실 문제가 크게 감소하여 tanh나 logistic활성화 함수같은 수렴성을 가진 활성화함수를 사용할 수 있게 됨\n",
    "- 또한 가중치 초기화에 네트워크가 훨씬 덜 민감해지게 됨\n",
    "    - 그래서 훨씬 더 큰 learning rate를 사용해서 학습 과정의 속도 자체를 크게 높일 수 있음\n",
    "- 그리고 배치 정규화는 규제와같은 역할을 하여 드롭아웃과 같은 다른 규제 기법의 필요성을 줄여줌\n",
    "- But 배치 정규화는 모델을 정규화하는 층이 추가되는 것이므로, 복잡도를 키우게되며, 실행 시간면에서 손해가 될 수 있음\n",
    "    - 층마다 추가되는 계산이 신경망의 예측을 느리게 함\n",
    "    - 이 문제를 피하기 위해 훈련 후 이전 층과 배치 정규화 층을 합쳐서 전체 층의 수를 줄이고 실행속도 저하를 피할 수 있음\n",
    "        - 예를 들어 이전 층이 XW + b를 계산한다면 배치 정규화층은 gamma X (XW + b - mu)/sigma + beta를 계산함\n",
    "            - 이를 단순화 하면 XW' + b'의 형태로 단순화 될 수 있음\n",
    "            - 어짜피 예측시에는 미리 계산해놓은 mu와 beta를 쓰기 때문에, 이전 층의 가중치와 편향인 W와 b를 W'와 b'로 바꾸면 하나의 층에서 배치 정규화도 동시에 실행 가능한 거심!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 케라스로 구현해보기\n",
    "- 은닉층의 활성화 함수 전이나 후에 BatchNormalization층을 추가하면 됨\n",
    "    - 모델의 첫 층으로 배치 정규화 층을 추가하는 방법도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# MNIST 데이터 가정\n",
    "# 활성화함수 이후에 배치 정규화 층을 추가하는 방식\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation = \"elu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation = \"elu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 처럼 그닥 깊지 않고 단순한 신경망에서는 큰 위력을 발휘하지 못할 수 있으나, 깊은 신경망으로 들어갈 수록 큰 차이를 만들어냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 보면 배치 정규화층은 매 입력마다 네개의 파라미터를 추가하므로 784 * 4, 300 * 4, 100 * 4 개수만큼의 파라미터를 학습하는 것으로 나옴\n",
    "- 그런데, gamma, beta, mu, sigma 중 mu와 sigma는 Non-trainabe파라미터 -> 역전파로 학습되지 않음\n",
    "- 첫 배치 정규화파라미터를 파라미터를 살펴보면, 두 개는 역전파로 훈련되고 두 개는 훈련이 되지 않음을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래서 케라스에서 배치 정규화 층을 만들 때, 훈련하는 동안 매 반복마다 케라스에서 호출될 두 개의 연산이 함께 생성됨\n",
    "    - 이 연산이 이동 평균을 업데이트 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'cond/Identity' type=Identity>,\n",
       " <tf.Operation 'cond_1/Identity' type=Identity>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 정규화 관련 논문 저자들은 일반적으로 배치 정규화층을 매 활성화 함수 이후가 다음이 아닌 **활성화 함수 통과 이전에**넣는 것이 좋다고 조언함\n",
    "    - 위 예제에서는 활성화 함수 통고 이후에 배치 정규화층이 추가된 형태\n",
    "    - 이건 근데 아직 논란의 여지가 있는 것이라서, 둘 다 실험해보고 더 적합한 것을 사용하는 것이 좋음\n",
    "- 활성화 함수 전에 배치 정규화층을 추가하려면 은닉층에서 활성화 함수를 지정하지말고, 배치 정규화 층 뒤에 별도의 층으로 추가해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터 가정\n",
    "# 활성화함수 이전에 배치 정규화 층을 추가하는 방식\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BatchNormalization클래스는 보통 파라미터를 조정하지 않아도 됨. 기본값이 잘 작동함\n",
    "    - But 가끔 **momentum인자**를 조정해야할 때가 있음\n",
    "- BatchNormalization이 이동평균을 업데이트할 때 해당 파라미터를 이용함\n",
    "    - 새로운 값 v(현재 배치에서 계산한 새로운 입력 평균 벡터나 표준편차 벡터)가 주어지면, v_hat을 업데이트\n",
    "        - v_hat = v_hat * momentum + v * (1-momentum)\n",
    "- 적절한 모멘텀값은 일반적으로 1에 가까움\n",
    "    - 보통 0.9, 0.99, 0.999 등\n",
    "    - 데이터가 크고 미니배치가 작으면 좀 더 1에 가깝게 잡음\n",
    "- 또 다른 중요 하이퍼파라미터는 axis임\n",
    "    - 정규화할 축을 정하며, 기본값은 -1임\n",
    "    - 즉, 다른 축을 따라 계산한 평균고 ㅏ표준편차로 마지막 축을 정규화함\n",
    "        - 무슨말이냐면, 입력 배치가 2D일 때 -> (샘플 개수, 특성 개수)이면, 각 입력특성이 배치에 있는 모든 샘플에 대해 계산한 평균과 표준편차를 기반으로 정규화됨\n",
    "        - 위 예시로 들면, 784개의 입력 특성마다 독립적으로 정규화 될 것임\n",
    "        - 위 예시에서는 Flatten층이 있는데, 만약 Flatten층 이전으로 BatchNormalization을 옮기면 3D가 될 것\n",
    "            - 즉 (샘플의 수, 높이, 너비)\n",
    "            - 이 때는 28개의 평균과 28개의 표준편차를 계산하고, 동일한 평균과 표준편차로 해당 열의 모든 픽셀을 정규화함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 정규화 층은 훈련 도중과 훈련 끝난 후 수행하는 계산이 다름\n",
    "- 훈련하는동안 배치 통계를 사용 -> 훈련 후 예측단계에서는 최종 통계를 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch NOrmalization은 DNN에서 매우 널리 사용하는 층이 되어있음\n",
    "- 또한 보통 모든 층 뒤에 배치 정규화가 되어있기 때문에 간혹 신경망 그림에 빠져있을 수 있음\n",
    "- 최근 연구결과들에서는 Fixup 가중치 초기화 방법을 사용하여 BatchNormalization없이 최고의 성능을 달성하기도했지만, 아직은 연구단계이기에 현업에서는 사용하는 것이 안정적일 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
