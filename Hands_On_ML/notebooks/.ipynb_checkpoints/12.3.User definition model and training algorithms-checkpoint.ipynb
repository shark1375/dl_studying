{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.3. 사용자 정의 모델과 훈련 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1. 사용자 정의 손실함수\n",
    "- MSE와 MAE에 오버피팅등의 문제, 이상치의 문제 등이 있을 떄, 사용할 수 있는 대표적인 손실함수는 Huber가 있은아, 공식 케라스 API 지원하지 않음. 아래와 같이 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, square_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NN에서 tf를 이용하여 모델을 구성할 떄는 되도록 위 예시처럼 모든 과정을 벡터화해야하며, tf 연산만 활용해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss = huber_fn, optimizer = \"namdam\")\n",
    "# model.fit(X_tr, y_tr, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런데 이렇게 임의로 지정한 함수는 모델을 저장할 때 문제가됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2. 사용자 정의 요소를 가진 모델의 저장/로드\n",
    "- 저장시에는 케라스가 함수 이름을 저장하므로, 사실 별 문제가 안됨\n",
    "- 다만, 모델을 로딩할 때 함수 이름과 실제 함수를 매핑할 딕셔너리를 전달해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"model.h5\", custom_object = {\"huber_fn\" : huber_fn})\n",
    "# # 따로 loss가 이거다, 저거다를 정의하지 않아도 되며, 이름만 일치하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(thr = 1.0): # 에러가 큰 지 작은 지 판단할 수 있는 threshold를 인자로 받는 생성함수를 만들어주기\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < thr\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = thr * tf.abs(error) - thr*2*0.5\n",
    "        return tf.where(is_small_error, square_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "# model.compile(loss = create_huber(2.0), optimizer = \"nadam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 threshold는 모델 저장시에 저장되지 않으며, 모델 로드시 threshold값을 지정하여 dic 전달 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"model.h5\", custom_objects = {\"huber_fn\" : create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이렇게 threshold를 지정하는 과정이 귀찮다면, 처음 loss function을 정의할 때 keras.losses.Loss 클래스를 상속하고 get_config()를 구현하는 방식이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold = 1.0, **kwargs): # 기본적인 하이퍼파라미터를 **Kwargs로 받은 인자를 부모 클래스 생성자에 전달\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred): # 모든 샘플의 손실을 계산하여 반환\n",
    "        error = y_true - y_pred\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold*2*0.5\n",
    "        return tf.where(is_small_error, square_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\" : self.threshold} # 하이퍼파라미터 이름과 같이 매핑된 딕셔너리를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 처럼 클래스를 선언하면, 컴파일 시 이 클래스의 인스턴스를 그대로 이용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss = HuberLoss(2.), optimizer = \"nadam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이렇게 컴파일한 모델을 저장하면 threshold도 함께 저장됨. 처음처럼 클래스 이름과 클래스 자체만 매핑해주면 됨\n",
    "- 여기서는 선언한 함수의 이름이 아니라 '클래스'명 자체를 매치시켜줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.model.load_model(\"model.h5\", custom_objects = {\"HuberLoss\" : HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.3. 활성화 함수, 초기화, 규제, 제한 커스터마이징\n",
    "- 위와 비슷한 방식으로 커스터마이징 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype = tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0]) + shape[1])\n",
    "    return tf.random.normal(shape, stddev = stddev, dtype = dtype)\n",
    "\n",
    "def my_l1_reguralizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeors_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만약 함수가 모델과 함께 저장해야할 하이퍼파라메터를 가지고 있다면, 적절한 클래스를 상속해야함\n",
    "    - ex : keras.regularizers.Regularizer, keras.constraints.Constraint, keras.initializers.Initializer, keras.layers.Layer 등\n",
    "- 예를들어 factor 하이퍼파라미터를 저장하는 l1규제를 클래스로 선언해보면 아래와 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizer 공식 문서를 참고해야함\n",
    "# 얘의 경우 부모 클래스에 생성자와 get_config()메서드가 정의되어있지 않음\n",
    "# 따라서 **kwargs를 호출하지 않았으며, get_config과정에서 **base_config를 받아오지 않아도 됨\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\" : self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- call()구현 대상 : 손실, 층, 모델\n",
    "- \\_\\_call__() 구현 대상 : 규제, 초기화 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
